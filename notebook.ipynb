{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-rfItV2zpqaC"
      },
      "source": [
        "### Set Up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "To7ZaL4R7nu5"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\", force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ROOT_DIR = \"/content/drive/MyDrive/content/drive\""
      ],
      "metadata": {
        "id": "34EkFwQsx7Ok"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hSjupYQDk3Dv"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w15wXc_Dk2pZ"
      },
      "outputs": [],
      "source": [
        "!pip install tweet-preprocessor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "STEO-k2B9-F5"
      },
      "outputs": [],
      "source": [
        "!pip install ekphrasis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Gms2yGanEZz"
      },
      "outputs": [],
      "source": [
        "!pip install imbalanced-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X5PbzTo9u8rW"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FLhjCIE61Xv2"
      },
      "outputs": [],
      "source": [
        "nltk.download('averaged_perceptron_tagger')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "id": "8s7lINOP7uRy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RPI3PD4-JS4t"
      },
      "outputs": [],
      "source": [
        "!pip install sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jd4_xdGBK82p"
      },
      "outputs": [],
      "source": [
        "!pip install tokenizers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "leUnXg6xp2Ca"
      },
      "source": [
        "### Loading Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xEAqTqxXh54b"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "from os.path import join\n",
        "col_list = [\"text\", \"label\"]\n",
        "\n",
        "def load_dataframe(folder, name, separator, encod):\n",
        "    path = join(ROOT_DIR, 'datasets', folder, name)\n",
        "    data = pd.read_csv(path, sep=separator, encoding=encod, engine='python', usecols=col_list)\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PqcqzM23rNnD"
      },
      "outputs": [],
      "source": [
        "data_train = load_dataframe(\"data1\", \"train.csv\", \"¦\", \"utf-8\")\n",
        "data_test = load_dataframe(\"data1\", \"test.csv\", \"¦\", \"utf-8\")\n",
        "data_validation = load_dataframe(\"data1\", \"validation.csv\", \"¦\", \"utf-8\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CgbAp6vub2LZ"
      },
      "source": [
        "### Save"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FHemAr-Jb4xo"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "def save_model(model, name):\n",
        "  path=ROOT_DIR +'/models/'+name+\".pickle\";\n",
        "  pickle.dump(model, open(path, 'wb'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9SXn-YQAr82w"
      },
      "source": [
        "### Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lTUwHBx3z13O"
      },
      "outputs": [],
      "source": [
        "from ekphrasis.classes.segmenter import Segmenter\n",
        "seg_tw = Segmenter(corpus=\"twitter\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gqvf9BbyptrN"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import stopwords, words\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "import re\n",
        "import string\n",
        "from string import punctuation, digits\n",
        "import preprocessor as p\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "\n",
        "tokenizer1 = TweetTokenizer()\n",
        "\n",
        "\n",
        "\n",
        "stop = set(stopwords.words('english'))\n",
        "punct = list(string.punctuation)\n",
        "stop.update(punct)\n",
        "\n",
        "def preprocess_tweets(text):\n",
        "    return p.clean(text)\n",
        "\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    final_text = []\n",
        "    text = text.split()\n",
        "    for i in text:\n",
        "        if i not in stop:\n",
        "            final_text.append(i)\n",
        "    return \" \".join(final_text)\n",
        "\n",
        "\n",
        "def delete_digits(text):\n",
        "    clean = text.translate(str.maketrans('', '', digits))\n",
        "    return clean\n",
        "\n",
        "\n",
        "def delete_punctuation(text):\n",
        "    translator = str.maketrans(string.punctuation, ' '*len(string.punctuation))\n",
        "    clean = text.translate(translator)\n",
        "    return clean\n",
        "\n",
        "\n",
        "def denoise_text(text):\n",
        "    text = preprocess_tweets(text)\n",
        "    text = text.lower()\n",
        "    text = delete_punctuation(text)\n",
        "    text = delete_digits(text)\n",
        "    return text\n",
        "\n",
        "def get_pos(word):\n",
        "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "    tag_dict = {\"J\": wordnet.ADJ,\n",
        "                \"N\": wordnet.NOUN,\n",
        "                \"V\": wordnet.VERB,\n",
        "                \"R\": wordnet.ADV}\n",
        "    return tag_dict.get(tag, wordnet.NOUN)\n",
        "\n",
        "\n",
        "def lemm(text):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    sar_list_lemmatizer = [lemmatizer.lemmatize(word, get_pos(word)) for word in text]\n",
        "    return \" \".join(sar_list_lemmatizer)\n",
        "\n",
        "\n",
        "\n",
        "def get_hashtags(text):\n",
        "    tokens = tokenizer1.tokenize(text)\n",
        "    n=len(tokens)\n",
        "    norm_arr=[]\n",
        "    \n",
        "    for i in range(n):\n",
        "      if tokens[i].startswith(\"#\"):\n",
        "          for word in seg_tw.segment(tokens[i]).split():\n",
        "            norm_arr.append(word)\n",
        "      else:\n",
        "        norm_arr.append(tokens[i])\n",
        "    return \" \".join(norm_arr)\n",
        "\n",
        "def get_vocab(text):\n",
        "    temp = text.copy()\n",
        "    vocab = text.apply(get_hashtags).apply(denoise_text).apply(remove_stopwords).apply(word_tokenize).apply(lemm)\n",
        "    vocab=vocab.tolist()\n",
        "    return vocab\n",
        "\n",
        "def get_vocab_bert(text):\n",
        "    temp = text.copy()\n",
        "    vocab = text.apply(get_hashtags).apply(denoise_text).apply(remove_stopwords)\n",
        "    vocab=vocab.tolist()\n",
        "    return vocab\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0pnL6tHgx3tB"
      },
      "source": [
        "### F1-metric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zsTZ3TMYJFEP"
      },
      "outputs": [],
      "source": [
        "import keras.backend as K\n",
        "import tensorflow as tf\n",
        "def f1_metric(y_true, y_pred):\n",
        "    y_pred = K.round(y_pred)\n",
        "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
        "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
        "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
        "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
        "\n",
        "    p = tp / (tp + fp + K.epsilon())\n",
        "    r = tp / (tp + fn + K.epsilon())\n",
        "\n",
        "    f1 = 2*p*r / (p+r+K.epsilon())\n",
        "    f1 = tf.where(tf.math.is_nan(f1), tf.zeros_like(f1), f1)\n",
        "    return K.mean(f1)\n",
        "\n",
        "def f1_loss(y_true, y_pred):\n",
        "    \n",
        "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
        "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
        "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
        "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
        "\n",
        "    p = tp / (tp + fp + K.epsilon())\n",
        "    r = tp / (tp + fn + K.epsilon())\n",
        "\n",
        "    f1 = 2*p*r / (p+r+K.epsilon())\n",
        "    f1 = tf.where(tf.math.is_nan(f1), tf.zeros_like(f1), f1)\n",
        "    return 1 - K.mean(f1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GnlSLzcSyZpG"
      },
      "source": [
        "### Threshold"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "mU-1q1NAAr34"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L4Z51qWu1eFB"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_curve\n",
        "import numpy as np\n",
        "def roc_curve_threshold(predicted_prob, y_labels):\n",
        "  fpr, tpr, thresholds = roc_curve(y_labels, predicted_prob)\n",
        "  # Youden’s J statistic.\n",
        "  J = tpr - fpr\n",
        "  ix = np.argmax(J)\n",
        "  print('Best Threshold=%f' % (thresholds[ix]))\n",
        "  # plot the roc curve for the model\n",
        "  plt.plot([0,1], [0,1], linestyle='--', label='No Skill')\n",
        "  plt.plot(fpr, tpr, marker='.', label='Logistic')\n",
        "  plt.scatter(fpr[ix], tpr[ix], marker='o', color='black', label='Best')\n",
        "  # axis labels\n",
        "  plt.xlabel('False Positive Rate')\n",
        "  plt.ylabel('True Positive Rate')\n",
        "  plt.legend()\n",
        "  # show the plot\n",
        "  plt.show()\n",
        "  \n",
        "  return thresholds[ix]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4R6UgfGD6Ev3"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import precision_recall_curve\n",
        "def pr_curve_threshold(yhat, testy):\n",
        "  precision, recall, thresholds = precision_recall_curve(testy, yhat)\n",
        "  # convert to f score\n",
        "  fscore = (2 * precision * recall) / (precision + recall)\n",
        "  # locate the index of the largest f score\n",
        "  ix = np.argmax(fscore)\n",
        "  print('Best Threshold=%f, F-Score=%.3f' % (thresholds[ix], fscore[ix]))\n",
        "  # plot the roc curve for the model\n",
        "  no_skill = len(testy[testy==1]) / len(testy)\n",
        "  plt.plot([0,1], [no_skill,no_skill], linestyle='--', label='No Skill')\n",
        "  plt.plot(recall, precision, marker='.', label='Logistic')\n",
        "  plt.scatter(recall[ix], precision[ix], marker='o', color='black', label='Best')\n",
        "  # axis labels\n",
        "  plt.xlabel('Recall')\n",
        "  plt.ylabel('Precision')\n",
        "  plt.legend()\n",
        "  # show the plot\n",
        "  plt.show()\n",
        "  return thresholds[ix]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jgA6ORNlyOsQ"
      },
      "outputs": [],
      "source": [
        "def threshold(value, opt_threshold):\n",
        "  if value>opt_threshold:\n",
        "    return 1\n",
        "  else:\n",
        "     return 0"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate"
      ],
      "metadata": {
        "id": "WyT_v8UYjKsJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_loaded_model(test_val, valid_val, name):\n",
        "  loaded_model = pickle.load(open(ROOT_DIR+'/models/'+name+'.pickle', 'rb'))\n",
        "  predicted = loaded_model.predict_proba(test_val)\n",
        "  opt_threshold = pr_curve_threshold(loaded_model.predict_proba(valid_val)[:, 1], data_validation[\"label\"])\n",
        "  predicted = [threshold(pred, opt_threshold) for pred in \n",
        "                predicted[:, 1]]\n",
        "  print(metrics.classification_report(data_test[\"label\"], predicted, digits=4))"
      ],
      "metadata": {
        "id": "6lhYK5AmjN53"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(test_val, valid_val, model):\n",
        "  predicted = model.predict(test_val)\n",
        "  opt_threshold = pr_curve_threshold(model.predict(valid_val), data_validation[\"label\"])\n",
        "  predicted = [threshold(pred, opt_threshold) for pred in \n",
        "                predicted]\n",
        "  print(metrics.classification_report(data_test[\"label\"], predicted, digits=4))"
      ],
      "metadata": {
        "id": "i4-4onu3qbem"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-GZ6mbtTx984"
      },
      "source": [
        "### Sampling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1vWPTrlkyeYJ"
      },
      "source": [
        "##### Random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mLXj0KQY-acb"
      },
      "outputs": [],
      "source": [
        "def random_over_sampling(data):\n",
        "    size = len(data[data['label'] == 0])\n",
        "    over_sampled_data = pd.concat([data[data['label'] == 0], data[data['label'] == 1].sample(size, replace=True)])\n",
        "    return over_sampled_data\n",
        "\n",
        "def random_under_sampling(data):\n",
        "    size = len(data[data['label'] == 1])\n",
        "    under_sampled_data = pd.concat([data[data['label'] == 0].sample(size), data[data['label'] == 1]])\n",
        "    return under_sampled_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AiivWWhnBs73"
      },
      "outputs": [],
      "source": [
        "under_random_train = random_under_sampling(data_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2gOW8x07xMz0"
      },
      "outputs": [],
      "source": [
        "under_random_train_text = get_vocab(under_random_train[\"text\"])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "under_random_train_text_bert = get_vocab_bert(under_random_train[\"text\"])"
      ],
      "metadata": {
        "id": "-frfF93nia9k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bFwAso9S6eqW"
      },
      "outputs": [],
      "source": [
        "over_random_train = random_over_sampling(data_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nFSDqyBiVfeZ"
      },
      "outputs": [],
      "source": [
        "over_random_train_text = get_vocab(over_random_train[\"text\"])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "over_random_train_text_bert = get_vocab(over_random_train[\"text\"])"
      ],
      "metadata": {
        "id": "h0p2JBUTiiKD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fjxhd9ayZZx"
      },
      "source": [
        "##### SMOTE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WuwIaRjlnMgP"
      },
      "outputs": [],
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "def smote_oversampling(data_text, data_label):\n",
        "  oversample = SMOTE()\n",
        "  X, y = oversample.fit_resample(data_text, data_label)\n",
        "  return X, y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_Vb5gKLynZ7"
      },
      "source": [
        "##### SMOTE + RandomUnderBalancing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AiHLhKQOytNX"
      },
      "outputs": [],
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from imblearn.pipeline import Pipeline\n",
        "def smote_over_undersampling(data_text, data_label):\n",
        "  over = SMOTE(sampling_strategy=0.5)\n",
        "  under = RandomUnderSampler(sampling_strategy=0.5)\n",
        "  steps = [('o', over), ('u', under)]\n",
        "  pipeline = Pipeline(steps=steps)\n",
        "  X, y = pipeline.fit_resample(data_text, data_label)\n",
        "  return X, y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mlll_07c22K9"
      },
      "source": [
        "### Tf.idf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v578w-5aEgod"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6-uW4C-cER9C"
      },
      "outputs": [],
      "source": [
        "def tf_idf_vectorizer(corpus):\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    X = vectorizer.fit_transform(corpus)\n",
        "    return X, vectorizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lyMUMLEP2-dH"
      },
      "source": [
        "### Count vectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jSICpLmjEi0y"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z5lIzsjIEdoF"
      },
      "outputs": [],
      "source": [
        "def count_vectorizer(corpus):\n",
        "    vectorizer = CountVectorizer(stop_words='english')\n",
        "    X = vectorizer.fit_transform(corpus)\n",
        "    return X, vectorizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSV0CKu83Iba"
      },
      "source": [
        "### Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "82dUE4wla10k"
      },
      "outputs": [],
      "source": [
        "from sklearn.naive_bayes import BernoulliNB\n",
        "from sklearn import metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wy-9yZT5amXx"
      },
      "outputs": [],
      "source": [
        "def naive_Bayes_Classif(x_train, x_test, y_train, y_test, name):\n",
        "  BNB = BernoulliNB()\n",
        "  \n",
        "  BNB.fit(x_train, y_train)\n",
        "  predicted = BNB.predict(x_test)\n",
        "  print(metrics.classification_report(y_test, predicted, digits=4))\n",
        "  save_model(BNB, name)\n",
        "  return BNB\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_text = get_vocab(data_test[\"text\"])\n",
        "valid_text = get_vocab(data_validation[\"text\"])\n",
        "train_text = get_vocab(data_train[\"text\"])"
      ],
      "metadata": {
        "id": "zeuOH8LySwjt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0bcRrci02_8J"
      },
      "source": [
        "#### Count Vectorizer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### No sampling"
      ],
      "metadata": {
        "id": "ZWJPmGx5mIw-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JZ2n6ztVvlva"
      },
      "outputs": [],
      "source": [
        "cv_train, cv = count_vectorizer(train_text)\n",
        "cv_test =  cv.transform(test_text)\n",
        "cv_valid =  cv.transform(valid_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iQ2Id-TW70sG"
      },
      "outputs": [],
      "source": [
        "naive_Bayes_Classif(cv_train, cv_test, data_train[\"label\"], data_test[\"label\"], \"bayes_naiv\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_loaded_model(cv_test, cv_valid, 'bayes_naiv')"
      ],
      "metadata": {
        "id": "pQ7wNApskSIc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### SMOTE"
      ],
      "metadata": {
        "id": "48TzNmejmHBv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-8-12cDMzJmi"
      },
      "outputs": [],
      "source": [
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qsrROKuj3iqa"
      },
      "outputs": [],
      "source": [
        "over_smote_train_text, over_smote_train_label = smote_oversampling(cv_train,data_train[\"label\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t4Qr1Ldd3zcW"
      },
      "outputs": [],
      "source": [
        "naive_Bayes_Classif(over_smote_train_text, cv_test, over_smote_train_label, data_test[\"label\"], \"naive_bayes_smote\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tuyvayJ_zfEQ"
      },
      "outputs": [],
      "source": [
        "over_smote_under_train_text, over_smote_under_train_label = smote_over_undersampling(cv_train,data_train[\"label\"])\n",
        "print(Counter(over_smote_under_train_label))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lHXwCZHW0DHK"
      },
      "outputs": [],
      "source": [
        "naive_Bayes_Classif(over_smote_under_train_text, cv_test, over_smote_under_train_label, data_test[\"label\"], \"naive_bayes_smote_under\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Random Sampling"
      ],
      "metadata": {
        "id": "1UQKYZ5gmSz3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "23mw63qY1MAt"
      },
      "outputs": [],
      "source": [
        "cv_train, cv = count_vectorizer(under_random_train_text)\n",
        "cv_test =  cv.transform(test_text)\n",
        "naive_Bayes_Classif(cv_train, cv_test, under_random_train[\"label\"], data_test[\"label\"], \"naive_bayes_random_under\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RE9LCdlz2PjU"
      },
      "outputs": [],
      "source": [
        "cv_train, cv = count_vectorizer(over_random_train_text)\n",
        "cv_test =  cv.transform(test_text)\n",
        "naive_Bayes_Classif(cv_train, cv_test, over_random_train[\"label\"], data_test[\"label\"], \"naive_bayes_random_over\" )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D5oEPXm93Yz0"
      },
      "source": [
        "#### Tf.idf"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### No Sampling"
      ],
      "metadata": {
        "id": "80X3S8v5nJ-a"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XVadnJbJ3cSI"
      },
      "outputs": [],
      "source": [
        "tf_train, tf = tf_idf_vectorizer(train_text)\n",
        "tf_test =  tf.transform(test_text)\n",
        "tf_valid =  tf.transform(valid_text)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "naive_Bayes_Classif(tf_train, tf_test, data_train[\"label\"], data_test[\"label\"], \"bayes_naiv_tf\")"
      ],
      "metadata": {
        "id": "kyUw6ElaTA8p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Random Oversampling"
      ],
      "metadata": {
        "id": "zmMcC8MlnRjh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AvPNtydS4m_V"
      },
      "outputs": [],
      "source": [
        "tf_train, tf = tf_idf_vectorizer(under_random_train_text)\n",
        "tf_test =  tf.transform(test_text)\n",
        "tf_valid =  tf.transform(valid_text)\n",
        "naive_Bayes_Classif(tf_train, tf_test, under_random_train[\"label\"], data_test[\"label\"], \"naive_bayes_under_tf\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eU6bNAvk44kr"
      },
      "outputs": [],
      "source": [
        "tf_train, tf = tf_idf_vectorizer(over_random_train_text)\n",
        "tf_test =  tf.transform(test_text)\n",
        "tf_valid =  tf.transform(valid_text)\n",
        "naive_Bayes_Classif(tf_train, tf_test, over_random_train[\"label\"], data_test[\"label\"],\"naive_bayes_over_tf\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### SMOTE"
      ],
      "metadata": {
        "id": "TNYrB6tBnXR7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RYJtaSEo39ku"
      },
      "outputs": [],
      "source": [
        "over_smote_train_text, over_smote_train_label = smote_oversampling(tf_train,data_train[\"label\"])\n",
        "naive_Bayes_Classif(over_smote_train_text, tf_test, over_smote_train_label, data_test[\"label\"],\"naive_bayes_smote_tf\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JycGTcQQ4Cp2"
      },
      "outputs": [],
      "source": [
        "over_smote_under_train_text, over_smote_under_train_label = smote_over_undersampling(tf_train,data_train[\"label\"])\n",
        "naive_Bayes_Classif(over_smote_under_train_text, tf_test, over_smote_under_train_label, data_test[\"label\"],\"naive_bayes_smote_under_tf\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_loaded_model(tf_test, tf_valid, 'naive_bayes_smote_tf')"
      ],
      "metadata": {
        "id": "ZOy1VWIQmnl7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Logistic Regression"
      ],
      "metadata": {
        "id": "kKQmKOv1GIud"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression"
      ],
      "metadata": {
        "id": "QUDBCAJzJo7z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def logistic_regression_classif(x_train, x_test, y_train, y_test, name,value):\n",
        "  lr = LogisticRegression(verbose=1, solver='liblinear', C=0.05, penalty='l2',max_iter=1000, class_weight=value)\n",
        "  \n",
        "  lr.fit(x_train, y_train)\n",
        "  predicted = lr.predict(x_test)\n",
        "  print(metrics.classification_report(y_test, predicted, digits=4))\n",
        "  save_model(lr, name)\n",
        "  return lr"
      ],
      "metadata": {
        "id": "WVuYQhhPJrdV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_text = get_vocab(data_test[\"text\"])\n",
        "valid_text = get_vocab(data_validation[\"text\"])\n",
        "train_text = get_vocab(data_train[\"text\"])"
      ],
      "metadata": {
        "id": "29NEmd0jKA5Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QzFUPET4KA5P"
      },
      "source": [
        "#### Count Vectorizer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### No Sampling"
      ],
      "metadata": {
        "id": "vsztiVcwngzb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DazYYD_cKA5R"
      },
      "outputs": [],
      "source": [
        "cv_train, cv = count_vectorizer(train_text)\n",
        "cv_test =  cv.transform(test_text)\n",
        "cv_valid =  cv.transform(valid_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "smQ045-NKA5S"
      },
      "outputs": [],
      "source": [
        "logistic_regression_classif(cv_train, cv_test, data_train[\"label\"], data_test[\"label\"], \"logistic_regression_balanced\",'balanced')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_loaded_model(cv_test, cv_valid, 'logistic_regression_balanced')"
      ],
      "metadata": {
        "id": "7dQcUilunj_-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Random Sampling"
      ],
      "metadata": {
        "id": "qj2L5tiNnt0j"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ppYz9yVDKA5Y"
      },
      "outputs": [],
      "source": [
        "cv_train, cv = count_vectorizer(under_random_train_text)\n",
        "cv_test =  cv.transform(test_text)\n",
        "cv_valid =  cv.transform(valid_text)\n",
        "logistic_regression_classif(cv_train, cv_test, under_random_train[\"label\"], data_test[\"label\"], \"logistic_regression_under\", None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yuwt5P7DKA5Z"
      },
      "outputs": [],
      "source": [
        "cv_train, cv = count_vectorizer(over_random_train_text)\n",
        "cv_test =  cv.transform(test_text)\n",
        "cv_valid =  cv.transform(valid_text)\n",
        "logistic_regression_classif(cv_train, cv_test, over_random_train[\"label\"], data_test[\"label\"], \"logistic_regression_over\", None)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### SMOTE"
      ],
      "metadata": {
        "id": "IIBGQgiGnywa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "svhe1GJGKA5V"
      },
      "outputs": [],
      "source": [
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LonmAjPtKA5W"
      },
      "outputs": [],
      "source": [
        "over_smote_train_text, over_smote_train_label = smote_oversampling(cv_train,data_train[\"label\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bZiluZMrKA5X"
      },
      "outputs": [],
      "source": [
        "logistic_regression_classif(over_smote_train_text, cv_test, over_smote_train_label, data_test[\"label\"], \"logistic_regression_smote\", None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l9HCUBJVKA5X"
      },
      "outputs": [],
      "source": [
        "over_smote_under_train_text, over_smote_under_train_label = smote_over_undersampling(cv_train,data_train[\"label\"])\n",
        "print(Counter(over_smote_under_train_label))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZqHUMN9dKA5Y"
      },
      "outputs": [],
      "source": [
        "logistic_regression_classif(over_smote_under_train_text, cv_test, over_smote_under_train_label, data_test[\"label\"], \"logistic_regression_smote_under\", None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NtMiCSQ1pM3I"
      },
      "source": [
        "#### Tf.idf"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### No Sampling"
      ],
      "metadata": {
        "id": "_tmgzOD1oLdb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aToWcjE0pM3J"
      },
      "outputs": [],
      "source": [
        "tf_train, tf = tf_idf_vectorizer(train_text)\n",
        "tf_test =  tf.transform(test_text)\n",
        "tf_valid =  tf.transform(valid_text)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "logistic_regression_classif(tf_train, tf_test, data_train[\"label\"], data_test[\"label\"], \"logistic_regression_balanced_tf\",'balanced')"
      ],
      "metadata": {
        "id": "66st0cpRpM3J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Random Sampling"
      ],
      "metadata": {
        "id": "WiSVkKfdoSbv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ewBBn5DNpM3J"
      },
      "outputs": [],
      "source": [
        "tf_train, tf = tf_idf_vectorizer(under_random_train_text)\n",
        "tf_test =  tf.transform(test_text)\n",
        "tf_valid =  tf.transform(valid_text)\n",
        "logistic_regression_classif(tf_train, tf_test, under_random_train[\"label\"], data_test[\"label\"], \"logistic_regression_under_tf\",None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pk93m2fLpM3J"
      },
      "outputs": [],
      "source": [
        "tf_train, tf = tf_idf_vectorizer(over_random_train_text)\n",
        "tf_test =  tf.transform(test_text)\n",
        "tf_valid =  tf.transform(valid_text)\n",
        "logistic_regression_classif(tf_train, tf_test, over_random_train[\"label\"], data_test[\"label\"],\"logistic_regression_over_tf\", None)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### SMOTE"
      ],
      "metadata": {
        "id": "cx_49kYHoW1F"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FH4RQ4chpM3K"
      },
      "outputs": [],
      "source": [
        "over_smote_train_text, over_smote_train_label = smote_oversampling(tf_train,data_train[\"label\"])\n",
        "logistic_regression_classif(over_smote_train_text, tf_test, over_smote_train_label, data_test[\"label\"],\"logistic_regression_smote_tf\", None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6rPmgQoMpM3K"
      },
      "outputs": [],
      "source": [
        "over_smote_under_train_text, over_smote_under_train_label = smote_over_undersampling(tf_train,data_train[\"label\"])\n",
        "logistic_regression_classif(over_smote_under_train_text, tf_test, over_smote_under_train_label, data_test[\"label\"],\"logistic_regression_smote_under_tf\",'balanced')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_loaded_model(cv_test, cv_valid, 'logistic_regression_smote_under_tf')"
      ],
      "metadata": {
        "id": "OOy7uGalobVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Word2Vec"
      ],
      "metadata": {
        "id": "rwdyA9_XQBpn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "from tensorflow.keras import models, layers, preprocessing as kprocessing\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import metrics\n",
        "import numpy as np\n",
        "from keras.callbacks import EarlyStopping\n",
        "from sklearn.utils import class_weight\n",
        "def w2v_embedding(train_text):\n",
        "  ## create list of lists of unigrams\n",
        "  lst_corpus = []\n",
        "  for string in train_text:\n",
        "    lst_words = string.split()\n",
        "    lst_grams = [\" \".join(lst_words[i:i+1]) \n",
        "                for i in range(0, len(lst_words), 1)]\n",
        "    lst_corpus.append(lst_grams)\n",
        "\n",
        "  ## detect bigrams and trigrams\n",
        "  bigrams_detector = gensim.models.phrases.Phrases(lst_corpus, \n",
        "                  delimiter=\" \".encode(), min_count=30)\n",
        "  bigrams_detector = gensim.models.phrases.Phraser(bigrams_detector)\n",
        "  trigrams_detector = gensim.models.phrases.Phrases(bigrams_detector[lst_corpus], \n",
        "              delimiter=\" \".encode(), min_count=30)\n",
        "  trigrams_detector = gensim.models.phrases.Phraser(trigrams_detector)\n",
        "\n",
        "  nlp = Word2Vec(lst_corpus, size=300,   \n",
        "            window=8, min_count=2, sg=1, iter=30, negative=5)\n",
        "  return nlp, lst_corpus, bigrams_detector, trigrams_detector\n",
        "def feature_engineering(lst_corpus, train_text):\n",
        "  tokenizer = kprocessing.text.Tokenizer(lower=True, split=' ', \n",
        "                     oov_token=\"NaN\", \n",
        "                     filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n')\n",
        "  tokenizer.fit_on_texts(lst_corpus)\n",
        "  dict_vocabulary = tokenizer.word_index\n",
        "  ## create sequence\n",
        "  lst_corpus = []\n",
        "  for string in train_text:\n",
        "    lst_words = string.split()\n",
        "    lst_grams = [\" \".join(lst_words[i:i+1]) \n",
        "                for i in range(0, len(lst_words), 1)]\n",
        "    lst_corpus.append(lst_grams)\n",
        "  lst_text2seq= tokenizer.texts_to_sequences(lst_corpus)\n",
        "  ## padding sequence\n",
        "  X_train = kprocessing.sequence.pad_sequences(lst_text2seq, \n",
        "                      maxlen=30, padding=\"post\", truncating=\"post\")\n",
        "  return tokenizer, dict_vocabulary, X_train\n",
        "def test_handling(test_text,bigrams_detector, trigrams_detector, tokenizer):\n",
        "  ## create list of n-grams\n",
        "  lst_corpus = []\n",
        "  for string in test_text:\n",
        "      lst_words = string.split()\n",
        "      lst_grams = [\" \".join(lst_words[i:i+1]) for i in range(0, \n",
        "                  len(lst_words), 1)]\n",
        "      lst_corpus.append(lst_grams)\n",
        "      \n",
        "  ## detect common bigrams and trigrams using the fitted detectors\n",
        "  lst_corpus = list(bigrams_detector[lst_corpus])\n",
        "  lst_corpus = list(trigrams_detector[lst_corpus])\n",
        "  ## text to sequence with the fitted tokenizer\n",
        "  lst_text2seq = tokenizer.texts_to_sequences(lst_corpus)\n",
        "\n",
        "  ## padding sequence\n",
        "  X_test = kprocessing.sequence.pad_sequences(lst_text2seq, maxlen=30,\n",
        "              padding=\"post\", truncating=\"post\")\n",
        "  return X_test\n",
        "def make_embedding_matrix(dic_vocabulary, nlp):\n",
        "  ## start the matrix (length of vocabulary x vector size) with all 0s\n",
        "  embeddings = np.zeros((len(dic_vocabulary)+1, 300))\n",
        "  for word,idx in dic_vocabulary.items():\n",
        "      ## update the row with vector\n",
        "      try:\n",
        "          embeddings[idx] =  nlp[word]\n",
        "      ## if word not in model then skip and the row stays all 0s\n",
        "      except:\n",
        "          pass\n",
        "  return embeddings\n",
        "def rn_w2v_model(embeddings):\n",
        "  ## input\n",
        "  x_in = layers.Input(shape=(30,))\n",
        "  ## embedding\n",
        "  x = layers.Embedding(input_dim=embeddings.shape[0],  \n",
        "                      output_dim=embeddings.shape[1], \n",
        "                      weights=[embeddings],\n",
        "                      input_length=30)(x_in)\n",
        "  ## 2 layers of bidirectional lstm\n",
        "  x = layers.Bidirectional(layers.LSTM(units=30, dropout=0.2, \n",
        "                          return_sequences=True))(x)\n",
        "  x = layers.Bidirectional(layers.LSTM(units=30, dropout=0.2))(x)\n",
        "  ## final dense layers\n",
        "  x = layers.Dense(64, activation='relu')(x)\n",
        "  y_out = layers.Dense(1, activation='sigmoid')(x)\n",
        "  ## compile\n",
        "  model = models.Model(x_in, y_out)\n",
        "  model.compile(loss='binary_crossentropy',\n",
        "                optimizer='adam', metrics=[f1_metric])\n",
        "\n",
        "  model.summary()\n",
        "  return model\n",
        "def w2v_train(model,X_train, y_train,  X_valid, y_valid):\n",
        "  y_train = np.array(y_train)\n",
        "  class_weights = class_weight.compute_class_weight(class_weight=\"balanced\", classes = np.unique(data_train[\"label\"]), y=np.array(data_train[\"label\"]))\n",
        "  print(class_weights)\n",
        "  class_weights={\n",
        "      0:class_weights[0],\n",
        "      1:class_weights[1]\n",
        "  }\n",
        "  early_stopping = EarlyStopping(\n",
        "      min_delta=0.01, # minimium amount of change to count as an improvement\n",
        "      patience=5, # how many epochs to wait before stopping\n",
        "      restore_best_weights=True,\n",
        "  )\n",
        "  #train with weights\n",
        "  training = model.fit(x=X_train, y=y_train, batch_size=32, \n",
        "                      epochs=20, shuffle=True, verbose=1, callbacks=[early_stopping],\n",
        "                      validation_data=(X_valid, y_valid), class_weight=class_weights)\n",
        "  ## plot loss and accuracy\n",
        "  metrics = [k for k in training.history.keys() if (\"loss\" not in k) and (\"val\" not in k)]\n",
        "  fig, ax = plt.subplots(nrows=1, ncols=2, sharey=True)\n",
        "  ax[0].set(title=\"Training\")\n",
        "  ax11 = ax[0].twinx()\n",
        "  ax[0].plot(training.history['loss'], color='black')\n",
        "  ax[0].set_xlabel('Epochs')\n",
        "  ax[0].set_ylabel('Loss', color='black')\n",
        "  for metric in metrics:\n",
        "      ax11.plot(training.history[metric], label=metric)\n",
        "  ax11.set_ylabel(\"Score\", color='steelblue')\n",
        "  ax11.legend()\n",
        "  ax[1].set(title=\"Validation\")\n",
        "  ax22 = ax[1].twinx()\n",
        "  ax[1].plot(training.history['val_loss'], color='black')\n",
        "  ax[1].set_xlabel('Epochs')\n",
        "  ax[1].set_ylabel('Loss', color='black')\n",
        "  for metric in metrics:\n",
        "      ax22.plot(training.history['val_'+metric], label=metric)\n",
        "  ax22.set_ylabel(\"Score\", color=\"steelblue\")\n",
        "  plt.show()\n",
        "  return model\n",
        "def w2v_test(model, X_test,y_test):\n",
        "  predicted_prob = model.predict(X_test)\n",
        "  opt_threshold = 0.5\n",
        "  #opt_threshold = roc_curve_threshold(predicted_prob, y_test)\n",
        "  #opt_threshold = pr_curve_threshold(predicted_prob, y_test)\n",
        "  predicted = [threshold(pred, opt_threshold) for pred in \n",
        "              predicted_prob]\n",
        "  print(metrics.classification_report(y_test, predicted, digits=4))"
      ],
      "metadata": {
        "id": "HdcMujf_QFMu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_text = get_vocab(data_train[\"text\"])\n",
        "test_text = get_vocab(data_test[\"text\"])\n",
        "valid_text = get_vocab(data_validation[\"text\"])"
      ],
      "metadata": {
        "id": "DLRGhD6dpVAJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp, lst_corpus, bigrams_detector, trigrams_detector = w2v_embedding(train_text)\n",
        "tokenizer, dic_vocabulary, X_train = feature_engineering(lst_corpus, train_text)\n",
        "X_test = test_handling(test_text,bigrams_detector, trigrams_detector, tokenizer)\n",
        "x_valid = test_handling(valid_text,bigrams_detector, trigrams_detector, tokenizer)"
      ],
      "metadata": {
        "id": "XZZ1uYwWQWLk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings=make_embedding_matrix(dic_vocabulary, nlp)\n",
        "w2v_model = rn_w2v_model(embeddings)\n",
        "w2v_model_trained = w2v_train(w2v_model, X_train, data_train[\"label\"], x_valid, data_validation[\"label\"])"
      ],
      "metadata": {
        "id": "h2HEWwSWRFp7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w2v_test(w2v_model_trained, X_test,data_test[\"label\"])"
      ],
      "metadata": {
        "id": "oPTPbDPTRhc3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_model(X_test, x_valid, w2v_model_trained)"
      ],
      "metadata": {
        "id": "Rc23-UNVq4Fy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_model(w2v_model_trained, \"word2vec_balanced_embed\")"
      ],
      "metadata": {
        "id": "KEhN4nunR0I9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9rStYX8AGO4S"
      },
      "source": [
        "### BERT (Sentence Transformers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GKLKWAoXJEE6"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "from keras.models import Sequential\n",
        "from keras.initializers import GlorotNormal\n",
        "from keras.layers import Dense, Dropout, Bidirectional, LSTM, Embedding, Input, BatchNormalization\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import metrics\n",
        "from keras import models\n",
        "from keras.callbacks import EarlyStopping\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.callbacks import LearningRateScheduler\n",
        "import tensorflow as tf\n",
        "from sklearn import model_selection, naive_bayes, svm\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from nltk.classify.scikitlearn import SklearnClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.utils import class_weight\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kVHRCT1EGTrT"
      },
      "outputs": [],
      "source": [
        "def sentence_transformers_embeddings(texts):\n",
        "  model = SentenceTransformer(\"all-mpnet-base-v2\")\n",
        "  embeddings = model.encode(texts)\n",
        "\n",
        "  return embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVBnUGO1JRlO"
      },
      "source": [
        "#### RN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z7zU67HYNvAD"
      },
      "outputs": [],
      "source": [
        "def rn_bert_sent_transf_model(embeddings, y_train):\n",
        "  input_size = len(embeddings[0])\n",
        "  model = Sequential()\n",
        "  initializer = GlorotNormal()\n",
        " \n",
        "  model.add(Dense(96, input_shape=(input_size, ), activation=\"relu\", kernel_initializer=initializer))\n",
        "  model.add(Dropout(0.5))\n",
        "  model.add(Dense(1, activation='sigmoid', kernel_initializer=initializer))\n",
        "  opt = tf.keras.optimizers.Adam(learning_rate=2e-5)\n",
        "  model.compile(loss='binary_crossentropy',\n",
        "                optimizer=opt, metrics=[f1_metric])\n",
        "  model.summary()\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u1u9o1IGRn6G"
      },
      "outputs": [],
      "source": [
        "def bert_sent_transf_train(model, y_train, X_train, X_valid, y_valid):\n",
        "  early_stopping = EarlyStopping(\n",
        "      min_delta=0.001, # minimium amount of change to count as an improvement\n",
        "      patience=5, # how many epochs to wait before stopping\n",
        "      restore_best_weights=True,\n",
        "  )\n",
        "  # class_weights = class_weight.compute_class_weight(class_weight=\"balanced\", classes = np.unique(y_train), y=np.array(y_train))\n",
        "  # class_weights={\n",
        "  #     0:class_weights[0],\n",
        "  #     1:class_weights[1]\n",
        "  # }\n",
        "  ## train\n",
        "  training = model.fit(x=X_train, y=y_train, batch_size=32, \n",
        "                      epochs=30, shuffle=True, verbose=1, callbacks=[early_stopping],\n",
        "                      validation_data=(X_valid, y_valid))#, class_weight=class_weights)\n",
        "  ## plot loss and accuracy\n",
        "  metrics = [k for k in training.history.keys() if (\"loss\" not in k) and (\"val\" not in k)]\n",
        "  fig, ax = plt.subplots(nrows=1, ncols=2, sharey=True)\n",
        "  ax[0].set(title=\"Training\")\n",
        "  ax11 = ax[0].twinx()\n",
        "  ax[0].plot(training.history['loss'], color='black')\n",
        "  ax[0].set_xlabel('Epochs')\n",
        "  ax[0].set_ylabel('Loss', color='black')\n",
        "  for metric in metrics:\n",
        "      ax11.plot(training.history[metric], label=metric)\n",
        "  ax11.set_ylabel(\"Score\", color='steelblue')\n",
        "  ax11.legend()\n",
        "  ax[1].set(title=\"Validation\")\n",
        "  ax22 = ax[1].twinx()\n",
        "  ax[1].plot(training.history['val_loss'], color='black')\n",
        "  ax[1].set_xlabel('Epochs')\n",
        "  ax[1].set_ylabel('Loss', color='black')\n",
        "  for metric in metrics:\n",
        "      ax22.plot(training.history['val_'+metric], label=metric)\n",
        "  ax22.set_ylabel(\"Score\", color=\"steelblue\")\n",
        "  plt.show()\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XbROHTFhTzuh"
      },
      "outputs": [],
      "source": [
        "def bert_sent_transf_test(model, X_test, y_test, X_valid, y_valid):\n",
        "  predicted_prob = model.predict(X_test)\n",
        "  opt_threshold = 0.5\n",
        "  #opt_threshold = pr_curve_threshold(model.predict(X_valid), y_valid)\n",
        "  predicted = [threshold(pred, opt_threshold) for pred in \n",
        "              predicted_prob]\n",
        "  accuracy = metrics.accuracy_score(y_test, predicted)\n",
        "  print(\"Accuracy:\",  round(accuracy,2))\n",
        "  print(metrics.classification_report(y_test, predicted, digits=4))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_train = get_vocab_bert(data_train[\"text\"])\n",
        "text_test = get_vocab_bert(data_test[\"text\"])\n",
        "text_valid = get_vocab_bert(data_validation[\"text\"])"
      ],
      "metadata": {
        "id": "AV3m6qmUulwR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c8YHG3mTJeld"
      },
      "outputs": [],
      "source": [
        "embeddings = sentence_transformers_embeddings(text_train)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_train = under_random_train_text_bert\n",
        "embeddings_labels = under_random_train[\"label\"]\n",
        "embeddings = sentence_transformers_embeddings(text_train)"
      ],
      "metadata": {
        "id": "Nh-HId8t4Hz_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CSLB7RX9rx3u"
      },
      "outputs": [],
      "source": [
        "embeddings, embeddings_labels  = smote_over_undersampling(embeddings, data_train[\"label\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vae9W_uxnzzU"
      },
      "outputs": [],
      "source": [
        "bert_sentence_transformer_model = rn_bert_sent_transf_model(embeddings, embeddings_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j7Qlv3eZTBTJ"
      },
      "outputs": [],
      "source": [
        "embeddings_valid = sentence_transformers_embeddings(text_valid)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gpm-NZm0oId6"
      },
      "outputs": [],
      "source": [
        "trained_bert_sent_transf_model = bert_sent_transf_train(bert_sentence_transformer_model, embeddings_labels, embeddings, embeddings_valid, data_validation[\"label\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gxtorC6yT6HX"
      },
      "outputs": [],
      "source": [
        "embeddings_test = sentence_transformers_embeddings(text_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ptmoX5_JjoP"
      },
      "outputs": [],
      "source": [
        "bert_sent_transf_test(trained_bert_sent_transf_model, embeddings_test, test_df[\"label\"], embeddings_valid, data_validation[\"label\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "99JdM4NBgkQP"
      },
      "outputs": [],
      "source": [
        "save_model(trained_bert_sent_transf_model, \"sentence_transf_smote_under\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g2OamllXg9C-"
      },
      "outputs": [],
      "source": [
        "trained_bert_sent_transf_model.save(ROOT_DIR+'/models/bert_rn_pr')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eli_bACihda3"
      },
      "outputs": [],
      "source": [
        "new_model = tf.keras.models.load_model(ROOT_DIR+'/models/bert_rn_pr', custom_objects={'f1_metric':f1_metric})\n",
        "new_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bert_sent_transf_test(new_model, embeddings_test, test_df[\"label\"], embeddings_valid, data_validation[\"label\"])"
      ],
      "metadata": {
        "id": "cc4HDXBR0UlG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adiC4hCbIL9S"
      },
      "source": [
        "### BERT Pretrained Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eAK8CyzeIPfF"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "from transformers import AutoModel, AdamW, AutoTokenizer, get_scheduler\n",
        "from torch.utils.data import DataLoader, TensorDataset, Dataset, RandomSampler\n",
        "import torch\n",
        "from sklearn.utils import class_weight\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "from tqdm.auto import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import f1_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "3ua91-z4JIT_"
      },
      "outputs": [],
      "source": [
        "#change model name for all the models you want to fine-tune\n",
        "embedding_config = {\n",
        "    \"model_name\": \"roberta-large\",\n",
        "    \"path\":ROOT_DIR+\"/models/roberta-large-new\",\n",
        "    \"max_length\":40,\n",
        "    \"batch_size\":16,\n",
        "    \"source\":\"HuggingFace\"\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TTnAL0OxJqUL"
      },
      "outputs": [],
      "source": [
        "model_config = {\n",
        "    \"learning_rate\" : 2e-5,\n",
        "    \"weight_decay\":0.01,\n",
        "    \"epochs\":20\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RSnqAef7KAKa"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xsgrHDb5KLpJ"
      },
      "outputs": [],
      "source": [
        "class_weights = class_weight.compute_class_weight(class_weight=\"balanced\", classes = np.unique(data_train[\"label\"]), y=np.array(data_train[\"label\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bkyczs8qKvBU"
      },
      "outputs": [],
      "source": [
        "class BertClassfierPytorch(nn.Module):\n",
        "  def __init__(self, input_size = 768, output_size = 1):\n",
        "    super().__init__()\n",
        "    print(input_size)\n",
        "    self.name = embedding_config[\"model_name\"]\n",
        "    self.tokenizer = AutoTokenizer.from_pretrained(embedding_config[\"model_name\"], use_fast=False)\n",
        "    self.embedder=AutoModel.from_pretrained(embedding_config[\"model_name\"])\n",
        "    self.dropout=nn.Dropout(0.5)\n",
        "    self.classifier=nn.Linear(input_size, output_size)\n",
        "    nn.init.xavier_uniform_(self.classifier.weight)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XZni1IeXMEEY"
      },
      "outputs": [],
      "source": [
        "def normalize(predicted):\n",
        "  opt_threshold=0.5\n",
        "  predicted = [threshold(pred, opt_threshold) for pred in \n",
        "              predicted]\n",
        "  return predicted\n",
        "\n",
        "def evaluate_predictions(predictions, labels):\n",
        "  metrics.confusion_matrix(labels, predictions)\n",
        "  print(metrics.classification_report(labels, predictions, digits=4))\n",
        "  return f1_score(labels, predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sbMaGtqiNADD"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, x_test, y_test):\n",
        "  x_test = model.tokenizer(x_test, padding  = \"max_length\", max_length = embedding_config[\"max_length\"], truncation = True, return_tensors = \"pt\")\n",
        "  test_data = TensorDataset(x_test[\"input_ids\"], x_test[\"attention_mask\"], torch.FloatTensor(y_test))\n",
        "\n",
        "  batch_size = embedding_config[\"batch_size\"]\n",
        "  test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
        "\n",
        "  preds=[]\n",
        "  model.eval()\n",
        "  for batch in test_dataloader:\n",
        "    aux_batch = {\n",
        "        \"input_ids\" : batch[0].to(device),\n",
        "        \"attention_mask\" : batch[1].to(device)\n",
        "    }\n",
        "\n",
        "    with torch.no_grad():\n",
        "      outputs = model.embedder(**aux_batch).pooler_output\n",
        "      outputs = model.classifier(model.dropout(outputs))\n",
        "\n",
        "    preds+=outputs.sigmoid().round().reshape(-1).tolist()\n",
        "  \n",
        "  predictions = normalize(preds)\n",
        "  return evaluate_predictions(predictions, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-fhzcdTQPgSL"
      },
      "outputs": [],
      "source": [
        "def train_model(model, x_train, y_train, x_valid, y_valid):\n",
        "  max_score = 0\n",
        "  \n",
        "  x_train = model.tokenizer(x_train, padding  = \"max_length\", max_length = embedding_config[\"max_length\"], truncation = True, return_tensors = \"pt\")\n",
        "  train_data = TensorDataset(x_train[\"input_ids\"], x_train[\"attention_mask\"], torch.FloatTensor(y_train))\n",
        "\n",
        "  batch_size = embedding_config[\"batch_size\"]\n",
        "  train_dataloader = DataLoader(train_data, batch_size=batch_size)\n",
        "\n",
        "  num_epochs = model_config[\"epochs\"]\n",
        "  num_training_steps = num_epochs * len(train_dataloader)\n",
        "\n",
        "  optimizer = AdamW(model.parameters(), lr = model_config[\"learning_rate\"], weight_decay = model_config[\"weight_decay\"])\n",
        "\n",
        "  lr_scheduler = get_scheduler(\n",
        "      \"linear\",\n",
        "      optimizer = optimizer,\n",
        "      num_warmup_steps = 0.2*num_training_steps,\n",
        "      num_training_steps = num_training_steps\n",
        "  )\n",
        "\n",
        "  print(\"Steps \",num_training_steps)\n",
        "\n",
        "  progress_bar = tqdm(range(num_training_steps))\n",
        "\n",
        "  model.train()\n",
        "  losses = []\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    print(\"Epoch: \", epoch+1)\n",
        "    for batch in train_dataloader:\n",
        "      aux_batch = {\n",
        "        \"input_ids\" : batch[0].to(device),\n",
        "        \"attention_mask\" : batch[1].to(device)\n",
        "      }\n",
        "\n",
        "      outputs = model.embedder(**aux_batch).pooler_output\n",
        "      outputs = model.classifier(model.dropout(outputs))\n",
        "\n",
        "      criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([5.2]).to(device))\n",
        "      loss = criterion(outputs, batch[2].to(device).reshape(-1,1))\n",
        "\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      optimizer.zero_grad()\n",
        "      lr_scheduler.step()\n",
        "      progress_bar.update(1)\n",
        "\n",
        "    losses.append(loss.tolist())\n",
        "    score =  evaluate(model, x_valid, y_valid)\n",
        "\n",
        "    if score>max_score:\n",
        "      max_score = score\n",
        "      torch.save(model.state_dict(), embedding_config[\"path\"]+\".pt\")  \n",
        "  return num_training_steps, losses\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eC3ZvOQ6UgvD"
      },
      "outputs": [],
      "source": [
        "text_train = get_vocab_bert(data_train[\"text\"])\n",
        "text_test = get_vocab_bert(data_test[\"text\"])\n",
        "text_valid = get_vocab_bert(data_validation[\"text\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VMCUAf15VQE1"
      },
      "outputs": [],
      "source": [
        "model = BertClassfierPytorch(input_size=1024)\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-3Az76k8Tl98"
      },
      "outputs": [],
      "source": [
        "num_training_steps, losses = train_model(model, text_train, data_train[\"label\"], text_valid, data_validation[\"label\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kZZpcHqwU8aw"
      },
      "outputs": [],
      "source": [
        "plt.plot(range(model_config[\"epochs\"]), losses)\n",
        "plt.ylabel(\"loss\")\n",
        "plt.xlabel(\"step\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iFBS4mZirvdz"
      },
      "outputs": [],
      "source": [
        "load_model = BertClassfierPytorch(input_size=1024)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CrC04G-K4sj0"
      },
      "outputs": [],
      "source": [
        "path = join(ROOT_DIR, 'models', \"roberta-large-new.pt\")\n",
        "load_model.load_state_dict(torch.load(path))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bhZAc92Nhlcb"
      },
      "outputs": [],
      "source": [
        "load_model.to(device)\n",
        "print(evaluate(load_model, text_test, data_test[\"label\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NpALKDquq4dH"
      },
      "source": [
        "### Ensemble Bert"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GWdGJViLIVX3"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "from transformers import AutoModel, AdamW, AutoTokenizer, get_scheduler\n",
        "from torch.utils.data import DataLoader, TensorDataset, Dataset, RandomSampler\n",
        "import torch\n",
        "from sklearn.utils import class_weight\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "from tqdm.auto import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import f1_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9XtjsSS6J3jN"
      },
      "outputs": [],
      "source": [
        "embedding_config = {\n",
        "    \"model_name\": \"bert-base-uncased\",\n",
        "    \"path\":ROOT_DIR+\"/models/bert-base-uncased\",\n",
        "    \"max_length\":40,\n",
        "    \"batch_size\":16,\n",
        "    \"source\":\"HuggingFace\"\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eG0Y7jUAJjnc"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dSFB6zf7rFFU"
      },
      "outputs": [],
      "source": [
        "text_train = get_vocab_bert(data_train[\"text\"])\n",
        "text_test = get_vocab_bert(data_test[\"text\"])\n",
        "text_valid = get_vocab_bert(data_validation[\"text\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zMaKP8Jfs4rj"
      },
      "outputs": [],
      "source": [
        "class BertPytorch(nn.Module):\n",
        "  def __init__(self,name=\"vinai/bertweet-large\", input_size = 1024, output_size = 1):\n",
        "    embedding_config = {\n",
        "      \"model_name\": name,\n",
        "      \"max_length\":40,\n",
        "      \"batch_size\":16,\n",
        "      \"source\":\"HuggingFace\"\n",
        "    }\n",
        "\n",
        "    self.embedding_config = embedding_config\n",
        "\n",
        "    model_config = {\n",
        "        \"learning_rate\" : 0.01,\n",
        "        \"weight_decay\":0.01,\n",
        "        \"epochs\":10\n",
        "    }\n",
        "    self.model_config = model_config\n",
        "    super().__init__()\n",
        "    self.name = embedding_config[\"model_name\"]\n",
        "    self.tokenizer = AutoTokenizer.from_pretrained(embedding_config[\"model_name\"], use_fast=False)\n",
        "    self.embedder=AutoModel.from_pretrained(embedding_config[\"model_name\"])\n",
        "    self.dropout=nn.Dropout(0.5)\n",
        "    self.classifier=nn.Linear(input_size, output_size)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    x_test = self.tokenizer(x, padding  = \"max_length\", max_length = embedding_config[\"max_length\"], \n",
        "                             truncation = True, return_tensors = \"pt\")\n",
        "    test_data = TensorDataset(x_test[\"input_ids\"], x_test[\"attention_mask\"])\n",
        "\n",
        "    batch_size = embedding_config[\"batch_size\"]\n",
        "    test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
        "\n",
        "    preds=[]\n",
        "    for batch in test_dataloader:\n",
        "      aux_batch = {\n",
        "          \"input_ids\" : batch[0].to(device),\n",
        "          \"attention_mask\" : batch[1].to(device)\n",
        "      }\n",
        "\n",
        "      with torch.no_grad():\n",
        "        outputs = self.embedder(**aux_batch).pooler_output\n",
        "        outputs = self.classifier(self.dropout(outputs))\n",
        "\n",
        "      preds+=outputs.reshape(-1).tolist()\n",
        "    \n",
        "    return preds\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TU8y46e2S8Mx"
      },
      "outputs": [],
      "source": [
        "class EnsembleModel(nn.Module):\n",
        "  def __init__(self, models_count):\n",
        "      super(EnsembleModel, self).__init__()\n",
        "      self.models_count=models_count\n",
        "      self.linear=nn.Linear(models_count, 128)\n",
        "      self.drop = nn.Dropout(0.2)\n",
        "      self.classifier=nn.Linear(128, 1)\n",
        "\n",
        "  \n",
        "  def forward(self, x):\n",
        "      x=self.linear(x)  \n",
        "      x=self.drop(x)\n",
        "      x=self.classifier(x)\n",
        "      return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s6Eb0qsnq8RX"
      },
      "outputs": [],
      "source": [
        "config = {\n",
        "    \"model_name\":\"ensemble-model-new\",\n",
        "    \"max_length\":40,\n",
        "    \"batch_size\":8,\n",
        "    \"learning_rate\":0.0001,\n",
        "    \"epsilon\": 1e-8,\n",
        "    \"weight_decay\": 1e-02,\n",
        "    \"epochs\":30\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize(predicted):\n",
        "  opt_threshold=0\n",
        "  predicted = [threshold(pred, opt_threshold) for pred in \n",
        "              predicted]\n",
        "  return predicted\n",
        "\n",
        "def evaluate_predictions(predictions, labels):\n",
        "  metrics.confusion_matrix(labels, predictions)\n",
        "  print(metrics.classification_report(labels, predictions, digits=4))\n",
        "  return f1_score(labels, predictions)\n",
        "\n",
        "def normalize_threshold(predicted, y_test):\n",
        "  opt_threshold = pr_curve_threshold(predicted, y_test)\n",
        "  predicted = [threshold(pred, opt_threshold) for pred in \n",
        "              predicted]\n",
        "  return predicted"
      ],
      "metadata": {
        "id": "BnXKSzvqLydr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YGqZ7vmstzCZ"
      },
      "outputs": [],
      "source": [
        "class TrainEnsembleModel():\n",
        "  def __init__(self, config): \n",
        "    self.config = config\n",
        "    self.models = [\n",
        "          (\"cardiffnlp/twitter-roberta-base-sentiment\", 768),\n",
        "          (\"cardiffnlp/twitter-roberta-base-hate\", 768),\n",
        "          (\"cardiffnlp/twitter-roberta-base-emotion\", 768),\n",
        "          (\"cardiffnlp/twitter-roberta-base-offensive\", 768),\n",
        "          (\"cardiffnlp/twitter-roberta-base-irony\", 768),\n",
        "          (\"cardiffnlp/twitter-roberta-base\", 768),\n",
        "          (\"vinai/bertweet-large\", 1024),\n",
        "          (\"ningkko/drug-stance-bert\", 768),\n",
        "          (\"ml4pubmed/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext_pub_section\", 768),\n",
        "          (\"dmis-lab/biobert-large-cased-v1.1-mnli\",  1024),\n",
        "          (\"roberta-large\", 1024),\n",
        "          (\"bert-base-uncased\",768)\n",
        "        ]\n",
        "\n",
        "  def get_models_out(self, x,x_label=None, x_test=None, y_test=None):\n",
        "      outs=[]\n",
        "      i=0\n",
        "      for model_name, model_dim in self.models:\n",
        "        index=1\n",
        "        if i==10 or i==11:\n",
        "            index=0\n",
        "        model_path = ROOT_DIR+\"/models/\"+model_name.split(\"/\",1)[index]+\".pt\"\n",
        "        new_model = BertPytorch(name=model_name, input_size = model_dim)\n",
        "        new_model.load_state_dict(torch.load(model_path, map_location = device))\n",
        "        new_model.to(device)\n",
        "        new_model.eval()\n",
        "        preds=new_model.forward(x)\n",
        "        outs.append(preds)\n",
        "        i+=1\n",
        "      outs=np.array(outs).transpose()\n",
        "      return outs\n",
        "\n",
        "  def train(self, y_train, x_valid, y_valid):\n",
        "      model = EnsembleModel(len(self.models))\n",
        "      model.to(device)\n",
        "      train_data = TensorDataset(torch.FloatTensor(x_train_ensemble), torch.FloatTensor(y_train))\n",
        "\n",
        "      batch_size = self.config[\"batch_size\"]\n",
        "      train_dataloader = DataLoader(train_data,shuffle=True, batch_size=batch_size)\n",
        "\n",
        "      num_epochs = self.config[\"epochs\"]\n",
        "      num_training_steps = num_epochs * len(train_dataloader)\n",
        "\n",
        "      optimizer = AdamW(model.parameters(), lr = self.config[\"learning_rate\"], \n",
        "                        weight_decay = self.config[\"weight_decay\"])\n",
        "\n",
        "      lr_scheduler = get_scheduler(\n",
        "          \"linear\",\n",
        "          optimizer = optimizer,\n",
        "          num_warmup_steps = 0.2*num_training_steps,\n",
        "          num_training_steps = num_training_steps\n",
        "      )\n",
        "\n",
        "      progress_bar = tqdm(range(num_training_steps))\n",
        "      max_score = 0 \n",
        "\n",
        "      model.train()\n",
        "\n",
        "      for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        print(\"Epoch: \", epoch+1)\n",
        "        for batch in train_dataloader:\n",
        "          aux_batch = {\n",
        "            \"x_train\" : batch[0].to(device),\n",
        "            \"label\" : batch[1].to(device)\n",
        "          }\n",
        "\n",
        "          outputs = model.forward(aux_batch[\"x_train\"])\n",
        "          criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([7.1]).to(device))\n",
        "          loss = criterion(outputs, aux_batch[\"label\"].reshape(-1,1))\n",
        "\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          optimizer.zero_grad()\n",
        "          lr_scheduler.step()\n",
        "          progress_bar.update(1)\n",
        "\n",
        "        score =  self.evaluate(model, x_valid_ensemble, y_valid)\n",
        "\n",
        "        import copy\n",
        "        if score>max_score:\n",
        "          max_score = score\n",
        "          best_model = copy.deepcopy(model)  \n",
        "      return max_score,best_model\n",
        "\n",
        "  def evaluate(self, model, x_test, y_test):\n",
        "      preds = self.get_predictions(x_test, model)\n",
        "      print(metrics.classification_report(y_test, preds, digits=4))\n",
        "      return f1_score(y_test, preds)\n",
        "\n",
        "  def get_predictions(self, x_test, model):\n",
        "      test_data = TensorDataset(torch.FloatTensor(x_test))\n",
        "\n",
        "\n",
        "      batch_size = self.config[\"batch_size\"]\n",
        "      test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
        "\n",
        "      preds=[]\n",
        "      model.eval()\n",
        "      for batch in test_dataloader:\n",
        "        aux_batch = {\n",
        "            \"x_test\" : batch[0].to(device)\n",
        "        }\n",
        "\n",
        "        with torch.no_grad():\n",
        "          outputs = model.forward(aux_batch[\"x_test\"])\n",
        "\n",
        "        preds+=outputs.sigmoid().round().reshape(-1).tolist()\n",
        "      \n",
        "      \n",
        "      return preds\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = TrainEnsembleModel(config)"
      ],
      "metadata": {
        "id": "y5t9oG-ARZGk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train_ensemble = trainer.get_models_out(text_train, data_train[\"label\"])"
      ],
      "metadata": {
        "id": "RH5MvwPIM2rX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_test_ensemble = trainer.get_models_out(text_test, data_test[\"label\"])"
      ],
      "metadata": {
        "id": "G6ASMTD-SZGn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_valid_ensemble = trainer.get_models_out(text_valid, data_validation[\"label\"])"
      ],
      "metadata": {
        "id": "a-nJBqHIN84o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_test_ensemble[0])"
      ],
      "metadata": {
        "id": "jL4l8NO2VpFM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f = open(\"x_train_ensemble.txt\", \"w\")\n",
        "for el in x_train_ensemble:\n",
        "  for i in el:\n",
        "    f.write(str(i)+\" \")\n",
        "  f.write(\"\\n\")\n",
        "f.close()"
      ],
      "metadata": {
        "id": "nydYGK5zMx__"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "acgrBhsarlBT"
      },
      "outputs": [],
      "source": [
        "trainer = TrainEnsembleModel(config)\n",
        "max_score, best_model = trainer.train(data_train[\"label\"], text_valid, data_validation[\"label\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zTc5ORZmxz6E"
      },
      "outputs": [],
      "source": [
        "torch.save(best_model.state_dict(), ROOT_DIR+\"/models/ensemble-model.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NP9enGcfubs3"
      },
      "outputs": [],
      "source": [
        "print(trainer.evaluate(best_model, x_test_ensemble, data_test[\"label\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### AdaBoost"
      ],
      "metadata": {
        "id": "iOa-xlp8sljx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "clf = AdaBoostClassifier(n_estimators=100)\n",
        "clf.fit(x_train_ensemble, data_train[\"label\"])"
      ],
      "metadata": {
        "id": "ArzDuu3zXAoP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds = clf.predict_proba(x_test_ensemble)\n",
        "probs = clf.predict_proba(x_valid_ensemble)\n",
        "opt_threshold = pr_curve_threshold(probs[:, 1], data_validation[\"label\"])\n",
        "predicted = [threshold(pred, opt_threshold) for pred in \n",
        "              preds[:, 1]]\n",
        "print(metrics.classification_report(data_test[\"label\"], predicted, digits=4))"
      ],
      "metadata": {
        "id": "-EIl-CU4syVT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds = clf.predict(x_test_ensemble)\n",
        "print(metrics.classification_report(data_test[\"label\"], preds, digits=4))"
      ],
      "metadata": {
        "id": "gYMsJiUWXjVk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save(clf, \"ada_boost_ensemble\")"
      ],
      "metadata": {
        "id": "Oc0j2yxjYAEK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Logistic Regression"
      ],
      "metadata": {
        "id": "455DFM-QtMcA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lr=logistic_regression_classif(x_train_ensemble, x_valid_ensemble, data_train[\"label\"], data_validation[\"label\"], \"logistic_regression_ensemble\",'balanced')"
      ],
      "metadata": {
        "id": "FROB3OkTZFwd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds = lr.predict_proba(x_test_ensemble)\n",
        "probs = lr.predict_proba(x_valid_ensemble)\n",
        "opt_threshold = pr_curve_threshold(probs[:, 1], data_validation[\"label\"])\n",
        "predicted = [threshold(pred, opt_threshold) for pred in \n",
        "              preds[:, 1]]\n",
        "print(metrics.classification_report(data_test[\"label\"], predicted, digits=4))"
      ],
      "metadata": {
        "id": "2h3iGmrpZ9I2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### KNN"
      ],
      "metadata": {
        "id": "cvPKC-yztWuY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(x_train_ensemble)\n",
        "\n",
        "X_train = scaler.transform(x_train_ensemble)\n",
        "X_test = scaler.transform(x_test_ensemble)\n",
        "X_valid = scaler.transform(x_valid_ensemble)"
      ],
      "metadata": {
        "id": "eL5VxVkaa3aT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "classifier = KNeighborsClassifier(n_neighbors=5)\n",
        "classifier.fit(X_train, data_train[\"label\"])"
      ],
      "metadata": {
        "id": "onJ6uzJHbEjr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds = classifier.predict(X_test)\n",
        "print(metrics.classification_report(data_test[\"label\"], preds, digits=4))"
      ],
      "metadata": {
        "id": "cRaXNGWlbJSJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save(classifier, \"knn_ensemble\")"
      ],
      "metadata": {
        "id": "z3Ds8TXNa33Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mpjFuYrBp_gA"
      },
      "source": [
        "### Plotting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KzFyIu69YGTt"
      },
      "source": [
        "#### Data Distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A08Vz_IHsMBN"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Le0_tsasKwL"
      },
      "outputs": [],
      "source": [
        "def tweet_distribution(count_adr, count_noadr, index_names, fig_name):\n",
        "  print(count_adr)\n",
        "  print(count_noadr)\n",
        "  plotdata = pd.DataFrame({\n",
        "    \"ADR\":count_adr,\n",
        "    \"no ADR\":count_noadr},\n",
        "    index=index_names)\n",
        "\n",
        "  plotdata.plot(kind=\"bar\",figsize=(15, 8))\n",
        "  plt.title(\"Tweets distribution\")\n",
        "  plt.xlabel(\"Set\")\n",
        "  plt.ylabel(\"Tweets\")\n",
        "  plt.savefig(fig_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mvfxtSI_oQUJ"
      },
      "outputs": [],
      "source": [
        "train_labels = data_train[\"label\"].tolist()\n",
        "test_labels = data_test[\"label\"].tolist()\n",
        "valid_labels = data_validation[\"label\"].tolist()\n",
        "\n",
        "tweet_distribution([train_labels.count(1),test_labels.count(1),valid_labels.count(1)], [train_labels.count(0),test_labels.count(0),valid_labels.count(0)],[\"Train\", \"Test\", \"Validation\"],\"dataset1_distribution.png\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QwniuotYNk0-"
      },
      "source": [
        "#### WordCloud"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7d11iM2iYEFp"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ihqoe0ajYlru"
      },
      "outputs": [],
      "source": [
        "def make_word_cloud(data, fig_name, typ):\n",
        "  examples = data[data['label']==typ]\n",
        "  text = get_vocab(data[\"text\"])\n",
        "  text = \" \".join(text)\n",
        "  word_cloud = WordCloud(collocations = False, background_color = 'white').generate(text)\n",
        "  plt.imshow(word_cloud, interpolation='bilinear')\n",
        "  plt.axis(\"off\")\n",
        "  plt.show()\n",
        "  plt.savefig(fig_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S_Qvee2Daa0t"
      },
      "outputs": [],
      "source": [
        "data_train[\"text\"] = get_vocab(data_train[\"text\"])\n",
        "make_word_cloud(data_train, \"positive_wordcloud.png\",1)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "-rfItV2zpqaC",
        "leUnXg6xp2Ca",
        "CgbAp6vub2LZ",
        "9SXn-YQAr82w",
        "0pnL6tHgx3tB",
        "GnlSLzcSyZpG",
        "-GZ6mbtTx984",
        "1vWPTrlkyeYJ",
        "Mlll_07c22K9",
        "lyMUMLEP2-dH",
        "rSV0CKu83Iba",
        "0bcRrci02_8J",
        "D5oEPXm93Yz0",
        "kKQmKOv1GIud",
        "NtMiCSQ1pM3I",
        "rwdyA9_XQBpn",
        "D7gQARAz9kMK",
        "OS8vkOpQ3RPh",
        "mpjFuYrBp_gA",
        "KzFyIu69YGTt"
      ],
      "name": "licenta.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}